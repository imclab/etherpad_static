<!doctype html>
<html lang="en">
<head>
<title>assessment-blog</title>
<meta charset="utf-8">
<style> * { font-family: arial, sans-serif;
font-size: 13px;
line-height: 17px; }ul.indent { list-style-type: none; }ol { list-style-type: decimal; }ol ol { list-style-type: lower-latin; }ol ol ol { list-style-type: lower-roman; }ol ol ol ol { list-style-type: decimal; }ol ol ol ol ol { list-style-type: lower-latin; }ol ol ol ol ol ol{ list-style-type: lower-roman; }ol ol ol ol ol ol ol { list-style-type: decimal; }ol  ol ol ol ol ol ol ol{ list-style-type: lower-latin; }</style>
</head>
<body>Assessment<br><br>This is a post about assessment of learning -- finding out what people know. I know what you think, how boring! But let me try to convince you otherwise. My main points are that there is a <strong>great opportunity to make assessment more meaningful</strong>; that this happening in lots of places already every day, not just in universities but also on web sites like Stack Overflow; and that we can hugely increase the value of open educational resources if we get it right.&nbsp;<br><br>I will briefly write about two things, the new types of assessment that are enabled by tools and practices common to the web, and the role of assessment within a broader badges infrastructure (where badges are signals for achievements or learning). And at the end, I will&nbsp; mention two initiatives that are starting to experiment and build prototypes based on these ideas for new assessments, and invite everyone to get involved.&nbsp;<br><br><strong>Is it going to be on the test?&nbsp;</strong><br><br>The best type of assessment is implicit - it is a by-product of meaningful activity, not an activity in itself. Assuming we consider physical exercise a meaningful activity, if we want to assess athletic achievement, we can think of ways to do so that most people would agree are objective measures. For example, when a&nbsp; measurement of 8,90m recognizes a long-jumper&#x27;s outstanding achievement, the measurement takes place independently of the jump itself and the connection between the measurement and the achievement is clearly established. A multiple choice exam that tries to evaluate a student&#x27;s&nbsp; ability to communicate in French creates a much more artificial setting, and consequently it is much harder to establish a clear link between&nbsp; the assessment and the ultimate learning goal (to speak French).&nbsp;<br><br>Too often in education, assessment is not connected to authentic learning, but has instead become its own purpose. Everyone with teaching experience in formal education will have heard the question, &quot;is this going to be on the test?&quot; and has most likely struggled with the appropriate answer as this simple question so easily highlights a fundamental disconnect. We &quot;teach&quot; in order to prepare students for life and work, but they &quot;learn&quot; to succeed on the test.&nbsp;<br><br>There is another fundamental problem with current assessment practices. A model where few experts assess the work of many non-experts doesn&#x27;t scale very well. Experts create bottlenecks. If open educational resources are to be useful to lots and lots of learners, then we need assessment practices than are meaningful and scale as part of the learning.<br><br><strong>Wouldn&#x27;t it be wonderful if we could identify certain meaningful tasks that people engage in, and recognize them in an authentic (non artificial) way that scales?</strong> The good news is that this is not just wonderful, but already common practice in online communities, where lots of users have to evaluate each others&#x27; contributions in order to achieve a common goal. The bad news is that as of today, these practices are rarely found in formal education.&nbsp;<br><br><strong>An example from the web</strong><br><br><em>[this section is good but needs one killer point that summarizes the idea and that you can bold]</em><br><br>Stack Overflow is an online question-and-answer platform for software developers. It is built on software that tracks every user&#x27;s activity and awards badges (yes, like boy scouts) for all kinds of contributions. There is a badge for the first question a user asks, a badge for answering a certain number questions, a badge that is awarded when the community determines one&#x27;s answer best addresses a particular question, and so on. Answers are voted up or down by the community, and a higher reputation gives users more power to influence an answer&#x27;s score (which determines if it is shown at the top of a long thread where it is easy to find, or near the bottom). New and less experienced developers benefit from the expertise and skills of their more experienced peers - as they learn from the voting and the discussion that typically accompanies the separation of good from bad answers. It&#x27;s important to note that Stack Overflow doesn&#x27;t use these peer review and -assessment practices, and badges, to recognize learning achievements. It uses them to identify high quality answers. What we think of as assessment is simply a way for Stack Overflow to improve what it does - help software developers get good answers to their questions. Nevertheless, we can already learn quite a lot about users&#x27; participation, interests, and draw some conclusions regarding their expertise, by looking at the badges they have collected.&nbsp;<br><br><em>[add something else about the fact this also happens in open source community informally -- people do (peer) code review -- which is both mentoring and assessment?]</em><br><br>What are the key messages for learning assessments:<br><br><ul class="bullet"><li>Communities are able to evaluate quality internally, using peer review, aggregating&nbsp; opinions, considering reputation and different levels of trust</li><li>Feedback loops enable improvements and learning for everyone, not just the active participants, but also lurkers</li><li>Assessment can be much more granular, and is driven by the needs or goals of the community (not a separate activity)</li><li>Because there can be many more and smaller achievements, new forms of recognizing and signaling them to others are needed.</li><li>It scales.&nbsp;</li></ul><br><strong>Storming the academy</strong><br><br>Not only can these forms of assessment improve the way we recognize skills, they also enable a <strong>completely new and distributed way of certifiying learning</strong> and expressing it to potential employers. What is needed for this to happen is a badges infrastructure that is decentralized, controled by users and driven by the types of assessment discussed above.&nbsp;<br><br>The badge becomes a signal of learning achievements, and within a secure badges infrastructure users can control how they manage these signals. Learners collect badges from different communities (think of this as individual credits from different Universities). They control where to store and how to display these badges, for example to potential employers only or on a&nbsp; public web-site. And there is a way to authenticate badges, to make sure&nbsp; that claims about achievements are in fact true. See Mark&#x27;s post about the details of this badges infrastructure for more information <a href="http&#x3a;&#x2F;&#x2F;commonspace&#x2e;wordpress&#x2e;com&#x2F;2010&#x2F;08&#x2F;12&#x2F;badges&#x2d;identity&#x2d;and&#x2d;you&#x2F;">http:&#x2F;&#x2F;commonspace.wordpress.com&#x2F;2010&#x2F;08&#x2F;12&#x2F;badges-identity-and-you&#x2F;</a><br><br>The P2PU &#x2F; Mozilla <strong>School of Webcraft will take these ideas about assessment and certification and test them for web developer training</strong>. We plan to use community voting and discussion models similar to those created by sites like Stack Overflow, and connect them to learning achievements. We have started identifying different types of activities and behaviors - those that express skills relevant to web developers - and are now working on formalizing them as badges, similar to the way Stack Overflow recognizes types of contributions.&nbsp;<br><br>In late September, Peer 2 Peer University -- in collaboration with Carnegie Foundation, Mozilla, and Shuttleworth&nbsp; Foundation -- is organizing a workshop about &quot;Learning assessment on the open web&quot; to identify other mechanisms used by open source communities that might be applicable to assessment of learning achievements. And a first prototype of the badges infrastructure will be presented at the Mozilla Drumbeat Festival in November - we plan to roll it out more broadly, with more partners, in early 2011.&nbsp;<br><br><br>Add links:&nbsp;<br><br><a href="http&#x3a;&#x2F;&#x2F;pad&#x2e;p2pu&#x2e;org&#x2F;assessment&#x2d;workshop">http:&#x2F;&#x2F;pad.p2pu.org&#x2F;assessment-workshop</a><br><a href="https&#x3a;&#x2F;&#x2F;www&#x2e;drumbeat&#x2e;org&#x2F;festival">https:&#x2F;&#x2F;www.drumbeat.org&#x2F;festival</a><br><br>============================================================<br><br>Additional paragraphs, I might use in connection with the above somewhere else. This would be the introduction. Probably not neceessary to read ... unless you want to!<br><br>Information<br><br>As effective users of the web, we constantly review and assess information - using a myriad of clues to help us determine how relevant something is to us and how much we trust it. Much of this happens automatic and we don&#x27;t think of our build-in sophisticated information filtering systems. Let&#x27;s look at a simple example. We generally trust a friend&#x27;s recommendation of a new band or movie, more than that made by a&nbsp; stranger. And even among our friends there are some whose taste in film we have especially learned to trust (or distrust) over time. The same is true for all pieces of information, content, knowledge we deal with. Depending on the importance of the information we might use different filtering mechanisms and employ them more consciously, but the basic mechanisms to determine usefulness and relevance are the same.&nbsp;<br><br>Moving things digital and online makes these skills more important, as we now have to filter and evaluate many more pieces of content and information as before, not only deciding which one&#x27;s we value more than others, but even deciding which ones to access in the first place. When there was one newspaper that covered all the news of the world, we could conceivably (pretend to) absorb the world&#x27;s event on a daily basis. But even a cursory glance at the news sources we can access online overwhelms.<br><br>Fortunately, many of the mechanisms we use in the offline world. still work online, for example the association of trust with certain brands or individuals. We may believe (or not) that journalists of the New York Time adhere to high professional standards, offline the same way as online. But there are new sources of information and knowledge that cannot rely on a similar trusted brand. When Wikipedia first made waves as a user edited Encyclopedia many questioned its ability to ensure quality of content and contributions. A new medium requires us to develop new ways of trusting it. This can create tension, when institutional ways of doing things are challenged and new norms developed (some professors allow their students to reference Wikipedia articles, and others don&#x27;t). But it is not just the old ways that have to adopt, sometimes completely new opportunities to determine usefulness of information emerge.&nbsp;<br><br>The emergence of some of these patterns online is creating a buzz about assessment and accreditation opportunities in the open education space. When communities like Wikipedia are able to define, test and recognize quality work of its contributors - should the same not be true for communities of learners? Or, when a community of software developers recognizes someone&#x27;s expertise and good judgement by entrusting her with the power to make changes to the group&#x27;s central software repository (so called &quot;commit&quot; rights), is that less of a recognition of expertise and skill than a degree in computer science? If the Internet lets us learn in the wild, what are the interfaces to the formal institutions that make sense, both for the informal learners who need some form of institutional validation or the institution that is reshaping its space in the open education landscape. These are just some of the exciting questions that the study of online communities sparks.&nbsp;<br><br><br></body>
</html>
